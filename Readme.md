# ğŸ“ NLP Preprocessing Pipeline

A complete NLP preprocessing workflow with separate notebooks for **data fetching** and **preprocessing**, designed for clean, structured, and reproducible text data preparation.

---

## ğŸ“Œ What is this project?

This project demonstrates an end-to-end NLP preprocessing pipeline:
- Fetch raw text data from an API
- Store it as a CSV (`data/raw_data.csv`)
- Apply text preprocessing in a separate notebook
- Prepare cleaned and vectorized text for downstream ML/NLP tasks

---

## â“ Why this project?

- Raw textual data is unstructured and noisy.  
- Preprocessing is crucial for building effective ML/NLP models.  
- Separate notebooks for **data fetching** and **preprocessing** make the workflow **modular and reproducible**.

---

## âš™ï¸ How does it work?

### ğŸ”‘ Workflow Steps:
1. **Data Fetching**
   - Fetch data from API in `notebooks/data_fetch.ipynb`  
   - Save as `data/raw_data.csv`

2. **Preprocessing**
   - Load `raw_data.csv` in `notebooks/preprocessing.ipynb`  
   - Perform text cleaning, tokenization, stopword removal, stemming, lemmatization, vectorization  
   - Save cleaned data as `data/processed_data.csv`

3. **Optional: Feature Extraction & ML**
   - Use vectorized data for downstream tasks (classification, clustering, regression)

---

## ğŸ“‚ Repository Structure

```
NLP-Preprocessing-Movie/
â”‚-- src/
â”‚ â””â”€â”€ NLP-preprocessing.ipynb # Notebook: data fetching + preprocessing
â”‚
â”‚-- data/
â”‚ â”œâ”€â”€ movie.csv # Raw movie dataset
â”‚ â””â”€â”€ processed_movie.csv # Preprocessed dataset
â”‚
â”‚-- reports/ # Optional: logs, plots, metrics
â”‚-- requirements.txt # Python dependencies
â”‚-- README.md
â”‚-- .gitignore
```

## ğŸ’»Tech Stack

- Python 3.8+
- pandas, numpy
- NLTK / spaCy for text preprocessing
- scikit-learn for vectorization